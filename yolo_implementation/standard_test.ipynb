{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A testing system of ´*.pt´-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based only on the `model.pt` file we can calculate stats from our standardized set [Roboflow project](https://universe.roboflow.com/ntnu-3oxpl/testset-qs9tl)\n",
    "This is just a hardcoded example set: The solution array is made by hand. The first column is the main building color and the second is all the colors present. By comparing the solution to the output of the test file we can make some stats.\n",
    "In order to use this test, just switch the `v12_larger.pt` file with your own model. Give the name in the box below and run the rest of the cells. The notebook will print the stats for the model at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"v12_larger.pt\"\n",
    "pathToModel = f\"data/models/{modelname}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the solution of the test set:\n",
    "It checks for the following things:\n",
    "1. What color the house in focus  is.\n",
    "2. How many houses there are and which types\n",
    "\n",
    "Other things: \n",
    "     It contains 20 images. Not used for anything in training our model. It is not part of the roboflow-dataset. It was only created for comparing models easily.\n",
    "\n",
    "|Labels|white|red|orange|black|yellow|darkgrey|grey|brown|blue|green|pink|\n",
    "|------|-----|---|------|-----|------|--------|----|-----|----|-----|----|\n",
    "|Amount|21|9|8|6|6|3|3|2|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = []\n",
    "\n",
    "# A hard-coded solution for the standard test set. First the main house, then all the present houses: \n",
    "solution.append([\"Main house color\", \"list of colors in picture\"])\n",
    "\n",
    "solution.append([\"white\", {'black':2, 'darkgrey':1, 'grey':1, 'white':1, 'yellow':1}])\n",
    "solution.append([\"orange\", {'orange':1, 'red':1, 'white':1, 'yellow':1}])\n",
    "solution.append([\"red\", {'orange':1, 'red':1, 'white':1}])\n",
    "solution.append([\"black\", {'black':1, 'orange':1, 'yellow':1, 'white':1}])\n",
    "solution.append([\"empty\", {}])\n",
    "solution.append([\"white\", {'white':1}])\n",
    "solution.append([\"yellow\", {'yellow':2}])\n",
    "solution.append([\"red\", {'red':4, 'white':1}])\n",
    "solution.append([\"orange\", {'orange':1}])\n",
    "solution.append([\"brown\", {'brown':1, 'grey':1, 'red':1}])\n",
    "solution.append([\"white\", {'white':3}])\n",
    "solution.append([\"darkgrey\", {'darkgrey':1, 'white':1}])\n",
    "solution.append([\"grey\", {'darkgrey':1, 'grey':1, 'red':1, 'white':1}])\n",
    "solution.append([\"black\", {'orange':1, 'black':2, 'white':2, 'red':1}])\n",
    "solution.append([\"black\", {'black':1}])\n",
    "solution.append([\"yellow\", {'orange':1, 'green':1, 'white':2, 'yellow':1}])\n",
    "solution.append([\"white\", {'white':4}])\n",
    "solution.append([\"brown\", {'brown':1}])\n",
    "solution.append([\"blue\", {'blue':1}])\n",
    "solution.append([\"white\", {'orange':2, 'pink':1, 'white':1}])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats returned from the test:\n",
    "1. Correct & false amount of main house predictions\n",
    "2. Correct & false amount of total houses predictions\n",
    "3. How many houses were missed\n",
    "\n",
    "Will not print out the images, but will print out the total stats. If you want to see the images, just remove the `#` in the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path, at the top of the notebook\n",
    "model = YOLO(pathToModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need some functions in order to retrieve the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradeResults(result):\n",
    "    predictions = []\n",
    "    \n",
    "    imgWidth = result[0].orig_shape[0]\n",
    "    imgHeight = result[0].orig_shape[1]\n",
    "    \n",
    "    for box in result[0].boxes:\n",
    "        predictedClass = model.names[int(box.cls)]\n",
    "        confidence = round(float(box.conf), 3)\n",
    "        \n",
    "        x, y, w, h = box.xywh[0]\n",
    "        x_offset = imgWidth/2 - (x)\n",
    "        y_offset = imgHeight/2 - (y)\n",
    "        offset =  x_offset**2 + y_offset**2\n",
    "        predictions.append((predictedClass, confidence, offset))\n",
    "    # Outputs list of guesses, confidence, and offset from center.\n",
    "    return predictions\n",
    "\n",
    "def chooseTheMiddleOne(predictions):\n",
    "    if len(predictions) == 0:\n",
    "        return \"empty\"\n",
    "    # Sorts by offset from center\n",
    "    predictions.sort(key=lambda x: x[2])\n",
    "    \n",
    "    minAcceptableOffset = 300\n",
    "    if predictions[0][2] > minAcceptableOffset**2:\n",
    "        return \"bad\"\n",
    "    \n",
    "    MiddleBoxClass = predictions[0][0]\n",
    "    return MiddleBoxClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting all the images in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyPredictions = [[\"Main building\", \"colors in picture\"]]\n",
    "for index in range(1, 21):\n",
    "    img = \"data/buildings/standardized_test/\" + str(index) + \".png\"\n",
    "    result = model(img, conf=0.35)\n",
    "    predictions = gradeResults(result)\n",
    "    colors = dict()\n",
    "    # Count the colors of the boxes\n",
    "    for prediction in predictions:\n",
    "        if prediction[0] in colors:\n",
    "            colors[prediction[0]] += 1\n",
    "        else:\n",
    "            colors[prediction[0]] = 1    \n",
    "    middleHousePrediction = chooseTheMiddleOne(predictions)\n",
    "    \n",
    "    MyPredictions.append([middleHousePrediction, colors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing a comparison table\n",
    "print(tabulate(zip(MyPredictions, solution), headers=[\"The prediction\", \"The solution\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stats:\n",
    "    def __init__(self, pred_, sol_):\n",
    "        self.pred = pred_\n",
    "        self.sol = sol_\n",
    "        \n",
    "        self.numberOfBuildings = 61\n",
    "        self.numberOfImages = 20\n",
    "                \n",
    "    def ComparePredAndSol(imgPred: dict, imgSol: dict):\n",
    "        correct = 0\n",
    "        missed = 0\n",
    "        tooMany = 0\n",
    "        \n",
    "        totalBuildings = 0\n",
    "        totalBuildingsPredicted = 0\n",
    "        \n",
    "        # Counting the total number of buildings predicted\n",
    "        for key in imgPred:\n",
    "            totalBuildingsPredicted += imgPred[key]\n",
    "        \n",
    "        # Counting the total number of buildings in the solution and comparing with prediction\n",
    "        for key in imgSol:\n",
    "            totalBuildings += imgSol[key]\n",
    "            \n",
    "            if key in imgPred:\n",
    "                if imgPred[key] == imgSol[key]:\n",
    "                    correct += imgSol[key]\n",
    "                else:\n",
    "                    correct += min(imgPred[key], imgSol[key])   \n",
    "        \n",
    "        missed = max(totalBuildings - totalBuildingsPredicted, 0)  \n",
    "        tooMany = max(0, totalBuildingsPredicted - totalBuildings)  \n",
    "        return correct, missed, tooMany   \n",
    "                \n",
    "    def calculateAccuracy(self):\n",
    "        # Main building calculation\n",
    "        self.CorrectMainBuildings = 0\n",
    "        for index in range(1, 21):\n",
    "            if self.pred[index][0] == self.sol[index][0]:\n",
    "                self.CorrectMainBuildings += 1\n",
    "                \n",
    "        self.CorrectBuildings = 0\n",
    "        self.MissedBuildings = 0\n",
    "        self.TooManyBuildings = 0\n",
    "        for index in range(1, self.numberOfImages + 1):\n",
    "            # Adding stats one image at a time\n",
    "            correct, missed, tooMany = Stats.ComparePredAndSol(self.pred[index][1], self.sol[index][1])\n",
    "            self.CorrectBuildings += correct\n",
    "            self.MissedBuildings += missed\n",
    "            self.TooManyBuildings += tooMany\n",
    "        \n",
    "    def printStats(self):\n",
    "        statTable = []\n",
    "        \n",
    "        mainBuildAccStat = str(100 * round(self.CorrectMainBuildings/self.numberOfImages, 3))\n",
    "        accStat = str(100 * round(self.CorrectBuildings/self.numberOfBuildings, 3))\n",
    "        missStat = str(100 * round(self.MissedBuildings/self.numberOfBuildings, 3))\n",
    "        tooManyStat = str(100 * round(self.TooManyBuildings/self.numberOfImages, 3))\n",
    "        \n",
    "        statTable.append([\"Main building accuracy\", str(self.CorrectMainBuildings), f\"{str(self.numberOfImages)} Images\", mainBuildAccStat])\n",
    "        statTable.append([\"Accuracy\", str(self.CorrectBuildings), f\"{str(self.numberOfBuildings)} Buildings\", accStat])\n",
    "        statTable.append([\"Missed\", str(self.MissedBuildings), f\"{str(self.numberOfBuildings)} Buildings\", missStat])\n",
    "        statTable.append([\"Too many\", str(self.TooManyBuildings), f\"{str(self.numberOfImages)} Images\", tooManyStat])                 \n",
    "        \n",
    "        header = [\"Statistic\", \"Result\", \"Out of\", \"Percentage\"]\n",
    "        \n",
    "        print(tabulate(statTable, headers=header))\n",
    "        \n",
    "        # Alternative way of printing without the tabulate method\n",
    "        # print(f\"Main building accuracy: {str(self.CorrectMainBuildings)}/{str(self.numberOfImages)}, {str(round(self.CorrectMainBuildings/self.numberOfImages, 3))}%\")\n",
    "        # print(f\"Accuracy: {str(self.CorrectBuildings)}/{str(self.numberOfBuildings)}, {str(round(self.CorrectBuildings/self.numberOfBuildings, 3))}%\")\n",
    "        # print(f\"Missed: {str(self.MissedBuildings)}/{str(self.numberOfBuildings)}, {str(round(self.MissedBuildings/self.numberOfBuildings, 3))}%\")\n",
    "        # print(f\"Too many: {str(self.TooManyBuildings)}/{str(self.numberOfImages)}, {str(round(self.TooManyBuildings/self.numberOfImages, 3))}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = Stats(MyPredictions, solution)\n",
    "stats.calculateAccuracy()\n",
    "stats.printStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the results, uncomment the following line\n",
    "That is to inspect the boxes\n",
    "The result should be shown inside a new folder named `outputs\\standardized_test`\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence_threshold = 0.25\n",
    "# model(\"data/buildings/standardized_test/\", conf=confidence_threshold, save=True, project=\"./outputs\", name=\"standardized_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
